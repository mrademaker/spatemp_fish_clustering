{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check For 64-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'64bit'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform\n",
    "platform.architecture()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from igraph import *\n",
    "import math\n",
    "from rtree import index\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from datetime import timedelta\n",
    "import datetime\n",
    "import pytz\n",
    "import fiona\n",
    "from shapely.geometry import LineString, Point, shape, mapping\n",
    "from dateutil import parser\n",
    "from fiona.crs import from_epsg\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn import metrics\n",
    "from scipy import spatial\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse import csgraph\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop graph from input points for calculating network distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class graphFromLinesAndPoints(object):\n",
    "    \n",
    "    def __init__(self,indexAndPoints):\n",
    "        self.nodeIDPoints =indexAndPoints\n",
    "        self.G = None#Graph()#nx.Graph()\n",
    "        self.K  = 10\n",
    "        self.epsilon = .001\n",
    "        self.allNodesMatch = {}\n",
    "        self.allNodesMatchWithLines = {}\n",
    "        self.rtIndex = None\n",
    "        self.weights = []\n",
    "        self.nodesTemporary = []\n",
    "        self.edgesTemporary = []\n",
    "        self.pntIndexToNodeID = {}\n",
    "        self.NodeIDtopntIndex = {}\n",
    "        self.pntLst = []\n",
    "        self.lowerDM = None\n",
    "        self.baseforsparse = []\n",
    "        self.pntMat = None\n",
    "        \n",
    "    def distance(self,x1,y1,x2,y2):\n",
    "        dx = x1-x2\n",
    "        dy = y1-y2\n",
    "        return math.sqrt(dx*dx+dy*dy)\n",
    "    \n",
    "    def distanceNode(self,node1,node2):\n",
    "        dx = node1[0]-node2[0]\n",
    "        dy = node1[1]-node2[1]\n",
    "        return math.sqrt(dx*dx+dy*dy)\n",
    "    \n",
    "    #need to contend with duplicate points...\n",
    "    def developGraphFromTriangulation(self):\n",
    "        self.pntLst = range(0,len(self.nodeIDPoints.keys()))\n",
    "        self.pntIndexToNodeID = {}\n",
    "        self.NodeIDtopntIndex = {}\n",
    "        print \"Remove Duplicates\"\n",
    "        for i,k in enumerate(self.nodeIDPoints.keys()):\n",
    "            self.pntLst[i] = self.nodeIDPoints[k]\n",
    "            \n",
    "        \n",
    "            \n",
    "        XC = np.array(self.pntLst)\n",
    "        self.pointMat = XC\n",
    "        print \"creating triangulation\"\n",
    "        tri = spatial.Delaunay(XC)\n",
    "        data_l = []\n",
    "        col_l = []\n",
    "        row_l = []\n",
    "        existingPairs = []\n",
    "        ids = []\n",
    "        \n",
    "        for t in tri.simplices:\n",
    "            \n",
    "            ds = spatial.distance.pdist(np.vstack([XC[t[0]],XC[t[1]],XC[t[2]]]),'euclidean')\n",
    "            ids.extend(t)\n",
    "            self.weights.append(ds[0])\n",
    "            self.weights.append(ds[1])\n",
    "            self.weights.append(ds[2])\n",
    "            pair = (t[0],t[1])\n",
    "            if not pair in existingPairs:\n",
    "                row_l.append(t[0])\n",
    "                col_l.append(t[1])\n",
    "                data_l.append(ds[0])\n",
    "                existingPairs.append(pair)\n",
    "            pair = (t[0],t[2])\n",
    "            if not pair in existingPairs:\n",
    "                row_l.append(t[0])\n",
    "                col_l.append(t[2])\n",
    "                data_l.append(ds[1])\n",
    "                existingPairs.append(pair)\n",
    "            pair = (t[1],t[2])\n",
    "            if not pair in existingPairs:\n",
    "                row_l.append(t[1])\n",
    "                col_l.append(t[2])\n",
    "                data_l.append(ds[2])\n",
    "                existingPairs.append(pair)\n",
    "                \n",
    "        self.baseforsparse = zip(row_l,col_l,data_l)\n",
    "        \n",
    "\n",
    "        print \"creating spatial index from unique points\"\n",
    "        triids = list(set(ids))\n",
    "        self.rtIndex = spatial.KDTree(XC[triids])\n",
    "        print \"match points to their index\"\n",
    "        \n",
    "        for i,k in enumerate(self.nodeIDPoints.keys()):\n",
    "            pnt = np.array(self.nodeIDPoints[k])\n",
    "            trq = self.rtIndex.query(pnt,k=1)\n",
    "            closestID = triids[trq[1]]\n",
    "            self.pntIndexToNodeID[closestID]=k\n",
    "            self.NodeIDtopntIndex[k]=closestID\n",
    "\n",
    "        print \"Sparse Matrix\"\n",
    "        self.G = coo_matrix((np.array(data_l),(np.array(row_l),np.array(col_l))),shape=(len(XC),len(XC)))\n",
    "        \n",
    "        print \"Lower Distance Matrix\"\n",
    "        self.lowerDM = np.tril(csgraph.dijkstra(self.G,directed=False))                     \n",
    "\n",
    "        \n",
    "    def getNodeDistances(self,nodeID1,nodeID2):\n",
    "        ind1 = self.NodeIDtopntIndex[nodeID1]\n",
    "        ind2 = self.NodeIDtopntIndex[nodeID2]\n",
    "        if ind1 > ind2:\n",
    "            dist = self.lowerDM[ind1][ind2]\n",
    "        else:\n",
    "            dist = self.lowerDM[ind2][ind1]\n",
    "        if dist == 0 or dist<0 or dist==np.Inf:\n",
    "            return None\n",
    "        else:\n",
    "            return dist\n",
    "    \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def getMaxKNearest(self,nodeID,K):\n",
    "\n",
    "        i = self.NodeIDtopntIndex[nodeID]\n",
    "\n",
    "        stuff = [x for x in self.lowerDM[i+1:,i] if x != np.inf]+[x for x in self.lowerDM[i,0:i] if x != np.inf]\n",
    "\n",
    "        stuff = sorted(stuff)\n",
    " \n",
    "        if max(stuff[0:K]) == 0:\n",
    "            return 1.0\n",
    "\n",
    "        return max(stuff[0:K])\n",
    "    \n",
    "    def getMaxKNearDict(self,keys, K):\n",
    "        maxLst = {}\n",
    "        for n in keys:\n",
    "            maxLst[n] = self.getMaxKNearest(n,K)\n",
    "        return maxLst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical clustering using linear weighted combination of Space Time and Attribute values\n",
    "\n",
    "## Based on distance matrix approach in Scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class st_hierarchical_net(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.K = 10\n",
    "        self.pointFeatureClass = \"\"#path to pointFeature Class\n",
    "        self.gdbPath = \"\"\n",
    "        self.outputPathFC = \"\"\n",
    "        self.outputNameFC = \"\"\n",
    "        self.timeScale = 86400 #used in the coversion to a nonlinear function\n",
    "        self.timeByScale = 3600 #Scale data to this value\n",
    "        self.timeCenter = 43200\n",
    "        self.attribScale = 1\n",
    "        self.idxTreeNodeID = {}\n",
    "        self.timeField = \"\"\n",
    "        self.attribField = \"\"\n",
    "        self.indexPointID = {}\n",
    "        self.maxKNearest = {}\n",
    "        self.df = None #pandas data frame\n",
    "        self.dm = None #sparse distance matrix\n",
    "        self.dfdm = None #pandas data frame of distance matrix\n",
    "        self.row_clusters = None\n",
    "        self.local_time_zone = None\n",
    "        self.weights = []\n",
    "        self.useridfield = None\n",
    "        self.indexidfield = None\n",
    "        self.indexuseridmatch = {}\n",
    "        self.labels = []\n",
    "        \n",
    "    def loadDataset(self):\n",
    "        \n",
    "\n",
    "        variables = ['NodeID', 'Time','Attribute']\n",
    "        self.labels = []\n",
    "        x_lst = []\n",
    "        idx = 0\n",
    "        nodeIds = {}\n",
    "\n",
    "        with fiona.open(self.gdbPath,'r',layer=self.pointFeatureClass) as coll:\n",
    "            i = 1\n",
    "            for row in coll:\n",
    "                s = shape(row['geometry'])\n",
    "                indexValue = None\n",
    "                if self.indexidfield:\n",
    "                    self.labels.append(row['properties'][self.indexidfield])\n",
    "                    indexValue = row['properties'][self.indexidfield]\n",
    "                else:\n",
    "                    self.labels.append(i)\n",
    "                    indexValue = i\n",
    "                    i+=1\n",
    "                    \n",
    "                nodeIds[indexValue]={}\n",
    "                dt = row['properties'][self.timeField]\n",
    "                x_lst.append([int(indexValue),dt,row['properties'][self.attribField]])\n",
    "                \n",
    "                self.indexPointID[indexValue] = (s.coords[0][0],s.coords[0][1])\n",
    "                if self.useridfield:\n",
    "                    self.indexuseridmatch[indexValue] = row['properties'][self.useridfield]\n",
    "                #idx+=1\n",
    "        self.internalGraph = graphFromLinesAndPoints(self.indexPointID)\n",
    "        self.internalGraph.developGraphFromTriangulation()\n",
    "        self.totalNetworkLength = sum(self.internalGraph.weights)\n",
    "        \n",
    "        X = np.array(x_lst)\n",
    "        self.df = pd.DataFrame(X, columns=variables, index=self.labels)\n",
    "        self.df['NodeID'] = self.df['NodeID'].astype(int)\n",
    "        print \"get max k\"\n",
    "        self.maxKNearest = self.internalGraph.getMaxKNearDict(self.internalGraph.NodeIDtopntIndex.keys(),self.K)\n",
    "        \n",
    "        print \"Creating distance matrix\"\n",
    "        print X\n",
    "        self.dm = pdist(X, self.calcDistanceMetric)\n",
    "        print \"create clusters\"\n",
    "        self.row_clusters = linkage(self.dm,method='average')\n",
    "        plt.title('Hierarchical Clustering Dendrogram')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Distance')    \n",
    "        \n",
    "    def calcDistanceMetric(self,x,y):\n",
    "        #assumed that there are three dimensions        \n",
    "        nearestMax = self.maxKNearest[int(x[0])]\n",
    "\n",
    "        dist = self.internalGraph.getNodeDistances(int(x[0]),int(y[0]))\n",
    "        if dist == None:\n",
    "            dist = self.totalNetworkLength /2.0\n",
    "\n",
    "        distSc = float(dist) / nearestMax#(float(self.totalNetworkLength)/2.0)\n",
    "        \n",
    "        t1 = x[1] - self.timeCenter #centered to noon, assuming seconds from midnight\n",
    "        t2 = y[1] - self.timeCenter #centered to noon, assuming seconds from midnight\n",
    "        \n",
    "        a1 = x[2]\n",
    "        a2 = y[2]\n",
    "        \n",
    "        \n",
    "        if self.weights[0]*x[0] == self.weights[0]*y[0] and self.weights[1]*t1==self.weights[1]*t2 and self.weights[2]*a1==self.weights[2]*a2:\n",
    "            #print \"equals\"\n",
    "            return 0.0\n",
    "\n",
    "        \n",
    "        tSec = abs(t1 - t2) \n",
    "        tRad = math.pi * (float(tSec)/self.timeScale) #convert time to radians, half unit circle\n",
    "        tCos = math.cos(tRad)\n",
    "        tDist = math.sqrt((t1*t1+t2*t2)-(2*t1*t2*tCos))#law of cosines to convert it to euclidean space\n",
    "        da = np.abs(x[2]-y[2])\n",
    "        daSc = da / self.attribScale\n",
    "        dtSc = float(tDist)/self.timeByScale #scaled to a value if in seconds 3600 is seconds from midnight\n",
    "        res = float(self.weights[0]*distSc + self.weights[1]*dtSc+ self.weights[2]*daSc)/(sum(self.weights))\n",
    "\n",
    "        if res >=0.0:\n",
    "            return res\n",
    "        else:\n",
    "            return 0.0\n",
    "    \n",
    "    def datetime_to_float(self,d):\n",
    "        epoch_utc = pytz.utc\n",
    "        epoch = datetime.datetime.utcfromtimestamp(0)\n",
    "        epochdt = epoch_utc.localize(epoch,is_dst=None)\n",
    "        total_seconds =  (d - epochdt).total_seconds()\n",
    "        \n",
    "        # total_seconds will be in decimals (millisecond precision)\n",
    "        return total_seconds\n",
    "\n",
    "    def float_to_datetime(self,fl):\n",
    "        return datetime.datetime.utcfromtimestamp(fl)\n",
    "    \n",
    "    def silohetteScore(self,n_clusters,printV=False):\n",
    "        outScores = []\n",
    "        sqdm = squareform(self.dm)\n",
    "        for n_c in n_clusters:\n",
    "            try:\n",
    "                cluster_labels = fcluster(self.row_clusters, n_c, criterion='maxclust')\n",
    "                silhouette_avg = silhouette_score(sqdm,cluster_labels,metric='precomputed')\n",
    "                outScores.append(silhouette_avg)\n",
    "                if printV:\n",
    "                    print \"For n_clusters =%s\"%n_c\n",
    "                    print \"The average silhouette_score is : %s\"%silhouette_avg\n",
    "            except:\n",
    "                pass\n",
    "        del sqdm\n",
    "        return outScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sth = st_hierarchical_net()\n",
    "sth.K = 7\n",
    "sth.pointFeatureClass = \"jimmy_2_6\"#\"comparisonDataset_20\"#\"allgeo_validsources_utm_random_la_sm\"# \"allgeo_validsources_utm_noident\"#path to pointFeature Class\n",
    "sth.gdbPath = r\"C:\\Users\\David\\OneDrive\\projects\\STHC\\dataset.gdb\"#r\"C:\\Users\\David\\OneDrive\\projects\\SNN-Net\\simWorking\\sim_db.gdb\"#r\"C:\\Users\\David\\OneDrive\\projects\\dissertationWork\\data\\allGeoTaggedMessages.gdb\"#r\"C:\\Users\\David\\OneDrive\\projects\\dissertationWork\\data\\allGeoTaggedMessages.gdb\"\n",
    "sth.outputPathFC = \"\"\n",
    "sth.outputNameFC = \"\"\n",
    "sth.timeScale = 86400\n",
    "sth.timeByScale = 3600#t #standard deviation may provide a good way to set this\n",
    "sth.attribScale = 1\n",
    "sth.timeField = \"TOTSEC\"#\"time\"#\"secfrmmid\" #assumed to be string since shapefiles cannot handle datetimes\n",
    "sth.attribField =\"ATTR\"#\"ATTRIB\"#\"attrib\"\n",
    "sth.weights=[.2,1.0,0.0]\n",
    "#sth.useridfield = \"ATTR\"\n",
    "#sth.indexidfield = \"Try\"\n",
    "print \"process data\"\n",
    "sth.loadDataset()\n",
    "#tracker[t][k]['sth'] = sth+\n",
    "#tracker[t][k]['scores']= sth.silohetteScore(cs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
